R^2^ = 1 - RSS / TSS
RSS	= сума от квадратите на разликите от модела
TSS	= сума от квадратите на разликите от средното

Когато използваме R^2^ за оценка на регресионен модел е възможно при добавянето на нови параметри на данните към модела Методът на най-малките да не намери по-ниска стойност за RSS. Това би задало коефициент 0 пред новата променлива в регресионният модел, и R^2^ остава същото. Така се получава, че всъщност R^2^ никога не намалява с добавянето на нови променливи.
Тъй като искаме нашият модел да използва възможно най-малко променливи и да постига най-висока точност, опитваме да "наказваме" употребата на повече променливи. Така извеждаме и оценката R^2^~adj~:
R^2^~adj~ = 1 - (1 - R^2^)(n - 1) / (n - k - 1)
n - броят на наблюденията
k - броят променливи
Тогава добавянето на нови променливи си струва, само ако те увеличават R^2^ достатъчно много.
В python можем да изчислим R^2^~adj~ така:
r2_adj = 1 - (1 - model.score(X, y)) * (len(y) - 1) / (len(y) - X.shape[1] - 1)
Където X са тренировъчните данни, а y етикетите им 